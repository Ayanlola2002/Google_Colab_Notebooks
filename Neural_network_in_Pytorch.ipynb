{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural network in Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulgureghian/Google_Colab_Notebooks/blob/master/Neural_network_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w6kBmzG7N_oW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Created by Paul A. Gureghian on 11/21/18. **\n",
        "\n",
        "** This Pytorch notebook uses the MNIST dataset, **\n",
        "** the Torchvision package, **\n",
        "** to build a deep neural network **"
      ]
    },
    {
      "metadata": {
        "id": "wN9EwJebNuu_",
        "colab_type": "code",
        "outputId": "71969fce-2e6d-4a14-b7fe-4318c98a8943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "### install pytorch and torchvision and helper\n",
        "\n",
        "!pip3 install -U -q http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install -U -q torchvision \n",
        "!pip3 install -U -q helper"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58ad8000 @  0x7f8d645802a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jlCal2HlQIPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### import packages\n",
        "\n",
        "import torch\n",
        "import helper\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import  datasets, transforms "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jr9NzDDMSa0H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### define a transform to normalize the data\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,)),])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BdtXmfhnbifR",
        "colab_type": "code",
        "outputId": "e49bcf83-7309-4764-b852-d30ba3c81477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "### download and load the training data\n",
        "\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "print(\"trainset: \", trainset)\n",
        "print('')\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "print(\"trainloader: \", trainloader)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "trainset:  Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Split: train\n",
            "    Root Location: /root/.pytorch/MNIST_data/\n",
            "    Transforms (if any): Compose(\n",
            "                             ToTensor()\n",
            "                             Normalize(mean=(0.5,), std=(0.5,))\n",
            "                         )\n",
            "    Target Transforms (if any): None\n",
            "\n",
            "trainloader:  <torch.utils.data.dataloader.DataLoader object at 0x7f164eb7c470>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3owBafvZeZqx",
        "colab_type": "code",
        "outputId": "ae54f91b-ea0f-4786-d195-f9092744b1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "### create an iterator with 'trainloader'\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(\"datatype of images: \", (type(images))) \n",
        "print('')\n",
        "print(\"shape of images: \", images.shape)\n",
        "print('')\n",
        "print(\"shape of labels: \", labels.shape)   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datatype of images:  <class 'torch.Tensor'>\n",
            "\n",
            "shape of images:  torch.Size([64, 1, 28, 28])\n",
            "\n",
            "shape of labels:  torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MAK-sS14jmBX",
        "colab_type": "code",
        "outputId": "14b80d0a-ba65-4c5a-c0e8-a3e43e35bb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "### plot an image \n",
        "\n",
        "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngAFhCQAG/wcYhqPXIoirYgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f164eb7ca58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DCqLg1-tlS5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### define a sigmoid activation function\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "seckXagsnNQC",
        "colab_type": "code",
        "outputId": "3a56f711-8458-4832-897e-c21dbc37d1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "### flatten the input images \n",
        "\n",
        "inputs = images.view(images.shape[0], -1)\n",
        "print(\"inputs: \", inputs)  \n",
        "print('')\n",
        "print(\"shape of inputs: \", inputs.shape) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs:  tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        ...,\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
            "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n",
            "\n",
            "shape of inputs:  torch.Size([64, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uglidzqlrrz7",
        "colab_type": "code",
        "outputId": "6f72cf13-ec9f-42f9-e4be-5416ce6c087f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "cell_type": "code",
      "source": [
        "### create parameters\n",
        "\n",
        "w1 = torch.randn(784, 256)\n",
        "print(\"w1: \", w1) \n",
        "print(\"shape of w1: \", w1.shape)\n",
        "print('')\n",
        "\n",
        "b1 = torch.randn(256) \n",
        "print(\"b1: \", b1)\n",
        "print(\"shape of b1: \", b1.shape) \n",
        "print('')\n",
        "\n",
        "w2 = torch.randn(256, 10)\n",
        "print(\"w2: \", w2)\n",
        "print(\"shape of w2: \", w2.shape)\n",
        "print('')\n",
        "\n",
        "b2 = torch.randn(10)\n",
        "print(\"b2: \", b2)\n",
        "print(\"shape of b2: \", b2.shape)  \n",
        "print('')\n",
        "\n",
        "hidden_layer = sigmoid(torch.mm(inputs, w1) + b1)\n",
        "print(\"h: \", hidden_layer)\n",
        "print(\"shape of h: \", hidden_layer.shape)\n",
        "print('')\n",
        "\n",
        "output_layer = torch.mm(hidden_layer, w2) + b2\n",
        "print(\"output: \", output_layer)\n",
        "print(\"shape of output: \", output_layer.shape)  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1:  tensor([[ 1.2199,  1.4494, -1.1372,  ...,  1.8372, -2.0748,  1.3353],\n",
            "        [-0.2881,  1.2630,  2.1758,  ...,  0.8986, -0.4582, -0.6778],\n",
            "        [-1.6745,  1.1183,  0.0305,  ...,  1.2758,  0.4194, -0.0006],\n",
            "        ...,\n",
            "        [ 1.6640,  1.2382,  1.1403,  ...,  0.7974,  0.1479, -1.0397],\n",
            "        [ 1.5042,  0.5258, -1.4306,  ..., -0.3837,  1.3524, -0.3824],\n",
            "        [ 0.1562, -0.0342, -3.1240,  ..., -0.8515, -0.0766, -2.5151]])\n",
            "shape of w1:  torch.Size([784, 256])\n",
            "\n",
            "b1:  tensor([-0.5528, -0.3425,  1.0052,  0.3224,  0.9466, -1.5941, -1.2889, -0.3850,\n",
            "        -0.0150,  0.3548,  0.7724,  0.1574,  0.2431,  0.2963,  0.1757,  0.9639,\n",
            "        -0.5071, -1.4315, -0.3186,  2.4429, -1.2757, -0.2557,  1.3777,  1.1925,\n",
            "        -1.0482,  1.8785, -0.5852, -0.2589,  0.5326, -0.0804,  0.3198,  1.0869,\n",
            "        -0.5033, -0.2433,  0.8652,  0.7421,  0.1678,  0.7637,  0.1121, -1.2947,\n",
            "        -0.3567, -0.1527, -1.2129, -0.3598,  0.1174,  2.8575, -1.4806, -0.7769,\n",
            "         0.8152, -1.2105, -1.1944, -1.4662,  0.6356,  1.1470,  0.2505, -0.0530,\n",
            "        -1.2658,  0.2225,  0.3762,  0.7725,  0.0868, -0.6640,  0.6472, -0.0682,\n",
            "        -0.4300,  1.2815,  0.5218,  0.0729, -0.1373, -1.0612, -0.5831,  0.0401,\n",
            "         0.7995, -0.3109, -0.5196,  0.8035,  0.6196,  1.1466,  0.0229, -0.7504,\n",
            "        -0.4999,  0.5658,  0.2983, -0.5325,  1.3329,  1.8663,  2.2416, -0.5333,\n",
            "         0.8969, -1.0590, -2.1129,  0.3091, -1.7188, -0.8575, -1.4960,  1.1395,\n",
            "         1.5214,  0.2783, -0.9122,  0.0876,  1.6873, -0.1476,  0.9762,  1.0852,\n",
            "        -0.6739,  0.8893, -0.2601,  0.5005, -0.9105, -0.7777, -1.9117, -0.1546,\n",
            "        -1.4618,  0.2944,  1.0414,  1.2370,  0.7749, -0.6319, -0.6258, -0.0418,\n",
            "         0.7295, -1.0188,  1.1392,  0.1238, -0.3809,  0.3541,  0.7200, -0.8909,\n",
            "         1.0446, -0.0490, -0.5242, -0.6532,  1.7011, -0.1975, -0.3774,  0.7339,\n",
            "        -0.8542, -0.6733,  0.3521,  0.2075,  1.8276, -0.0112,  0.0212,  0.1270,\n",
            "         0.7949,  1.2730,  0.7757,  2.4840,  0.0372,  0.3010, -0.3873,  0.0681,\n",
            "         0.7831, -0.9023, -0.0584, -0.8323,  0.7021,  0.7324,  0.8327,  0.6911,\n",
            "         0.0299, -2.2206,  0.8835,  0.3298, -0.7061,  0.3087, -0.9898,  0.3622,\n",
            "        -0.5776, -0.1918, -0.4373,  0.6814,  0.1117, -1.3650, -1.1755,  0.9379,\n",
            "        -1.2028, -1.2145, -0.0587, -0.4161, -0.4259, -1.4084,  0.9235, -0.1752,\n",
            "        -0.5568, -0.0053, -0.4814,  1.5418, -0.1930, -1.6904,  0.0775, -0.0649,\n",
            "         1.4437,  1.5120, -1.4072, -1.8779, -1.7800,  0.8142,  0.5347, -0.4928,\n",
            "         0.2790,  1.4843,  0.5879,  1.2496,  1.4084,  0.8628,  2.7201,  1.5250,\n",
            "        -0.4538, -1.6504, -2.3966, -0.2501, -1.8176, -0.6548, -0.3891,  0.8808,\n",
            "        -0.8952,  2.0281, -1.7181, -0.6154,  1.1374,  0.3789, -0.3946, -0.6552,\n",
            "         0.0892,  0.3376,  0.0579,  0.3833,  0.9276,  1.5416,  1.0126,  0.6889,\n",
            "         1.0409,  0.0147,  0.5757, -1.2669, -0.7605,  0.9790, -0.0065, -0.8087,\n",
            "        -1.5590,  1.4765, -0.1778, -1.0295, -0.9474, -1.5924,  0.6667,  0.7926,\n",
            "         1.0620, -0.0235, -0.2900,  0.3162,  2.3203, -0.3128,  0.6515, -0.6399])\n",
            "shape of b1:  torch.Size([256])\n",
            "\n",
            "w2:  tensor([[ 1.7112,  1.3391,  0.5554,  ..., -0.9248,  0.4743, -0.2025],\n",
            "        [-1.6255, -0.4248, -2.0208,  ..., -0.0690, -2.0661, -2.6400],\n",
            "        [ 0.1881,  0.3837, -0.6534,  ..., -0.0237, -0.1602, -1.0566],\n",
            "        ...,\n",
            "        [-1.6294,  0.7429, -1.1539,  ..., -0.8391, -1.4031,  1.4204],\n",
            "        [ 0.2074,  0.1965, -0.8624,  ..., -0.9371,  0.3530, -0.3346],\n",
            "        [ 0.1895,  0.0287, -1.2878,  ..., -0.3625,  0.4109, -0.2302]])\n",
            "shape of w2:  torch.Size([256, 10])\n",
            "\n",
            "b2:  tensor([ 2.1132, -0.9915, -0.5956, -0.2890, -0.1890,  1.1499,  0.3631, -1.2516,\n",
            "        -0.2234,  0.5117])\n",
            "shape of b2:  torch.Size([10])\n",
            "\n",
            "h:  tensor([[1.0000e+00, 9.9998e-01, 6.5119e-16,  ..., 1.0759e-30, 6.0874e-22,\n",
            "         9.9285e-01],\n",
            "        [1.0000e+00, 1.0000e+00, 4.2702e-08,  ..., 6.8740e-15, 4.7148e-12,\n",
            "         3.0111e-06],\n",
            "        [1.0000e+00, 9.1980e-01, 2.7517e-11,  ..., 3.3674e-31, 1.2656e-13,\n",
            "         1.0000e+00],\n",
            "        ...,\n",
            "        [1.0000e+00, 9.9996e-01, 1.8565e-06,  ..., 8.5407e-04, 2.1127e-07,\n",
            "         1.0000e+00],\n",
            "        [1.0000e+00, 6.2671e-02, 1.1609e-19,  ..., 1.1063e-21, 2.5086e-17,\n",
            "         2.6784e-03],\n",
            "        [1.0000e+00, 1.0000e+00, 4.7163e-11,  ..., 2.0778e-20, 2.0089e-15,\n",
            "         1.0000e+00]])\n",
            "shape of h:  torch.Size([64, 256])\n",
            "\n",
            "output:  tensor([[  3.5358,  -1.8737, -11.2276, -20.7976, -12.7199, -12.0636,   1.8949,\n",
            "         -16.6408, -18.5104,  -6.9461],\n",
            "        [  7.3419,  13.1877, -13.2446, -13.3720,  -4.0591,  -9.5533,   1.3569,\n",
            "          -7.3289,  -9.7300,   5.0183],\n",
            "        [ 11.9842,  -5.0715,  -7.6443, -11.9814,  -1.0596,  -7.8525,   7.5524,\n",
            "         -12.8630, -10.4562,  -2.1847],\n",
            "        [  4.2405,   5.9135, -12.8824, -14.5778,  -2.6653, -15.1776,  -3.5727,\n",
            "         -12.7243, -19.8246,  -4.6094],\n",
            "        [ 11.2177,   4.6598, -10.0598, -14.1252,   0.2525,  -7.0865,   2.3328,\n",
            "          -7.7099,  -7.7881,   5.8890],\n",
            "        [ 17.3431,  12.6408, -13.3224, -20.0440, -14.0514,  -4.4676,   2.9612,\n",
            "         -10.6017,  -7.4630,  -7.5868],\n",
            "        [  3.8920,   8.4683, -17.8943, -12.0076,  -1.1751, -10.5260,   1.0988,\n",
            "         -16.8415, -11.1844, -10.9879],\n",
            "        [  0.1802,   1.9686,  -7.7013, -23.6189,  -5.0297,  -4.7208,   6.3341,\n",
            "         -16.9808, -22.2741,  -8.4664],\n",
            "        [ 13.4620,  14.6305, -17.3182, -19.6749, -10.5468, -10.0008,  -4.0260,\n",
            "          -7.2009, -11.2456,   7.0575],\n",
            "        [ -1.6702,  -0.0356, -11.7197, -10.2135,  -5.3330, -15.5107,   3.1218,\n",
            "         -16.0497,  -8.7591,   4.7345],\n",
            "        [ 10.5435,   5.7369, -20.7757, -19.8994, -10.3991, -13.0566,  13.5295,\n",
            "         -23.6851,  -8.3117,  -3.3567],\n",
            "        [  1.2477,   7.1348, -20.0804, -14.9902,  -4.4616,  -9.9996,   0.7844,\n",
            "         -13.8100, -18.9674,  -3.3812],\n",
            "        [  7.6704,   9.1569, -18.9438, -11.1139,   4.2297, -15.7424,   4.7435,\n",
            "         -11.7264, -14.9082,   0.9746],\n",
            "        [ -4.3433,   1.2158, -14.0682, -18.3813,  -8.2078, -11.2485,   8.8210,\n",
            "          -8.7131,  -6.2350,  -6.2414],\n",
            "        [  7.4127,   9.0355, -13.5048, -22.6441,  -8.4890,  -6.5273,  -3.8418,\n",
            "         -17.1698, -18.4059,  -5.9614],\n",
            "        [ 13.5809,   3.6827, -13.3559, -16.1200,   3.4641, -12.7427,   9.4244,\n",
            "         -11.9199,  -8.7268,   2.2180],\n",
            "        [ 12.0709,   1.2767, -12.9302, -16.8155,   1.4703,  -8.1508,   2.6256,\n",
            "         -10.2122,  -6.6135,  -4.3776],\n",
            "        [  8.7541,   0.7334, -10.4103,  -8.8533,  -2.8355,  -6.1339,   3.3049,\n",
            "          -6.2776, -12.4561,  -2.8109],\n",
            "        [  4.6382,  -5.2328, -12.7969, -16.0670,  -1.4631,  -7.8343,  -2.1363,\n",
            "         -14.0118, -16.7804,  -4.5931],\n",
            "        [  1.7115,  11.1569,  -8.6620,  -7.9994,  -7.9245, -11.8561,   3.6501,\n",
            "          -8.8280, -18.7330,  -2.9588],\n",
            "        [  3.7946,   5.9024, -14.8787,  -9.6165,   2.7372,  -5.0352,  -5.4434,\n",
            "         -15.0759, -10.9301,   0.0508],\n",
            "        [ 17.6057,  10.5947, -20.6647, -17.9912, -11.5546,  -8.6343,   6.0676,\n",
            "         -10.7332,  -2.4744,   8.9754],\n",
            "        [ -2.3150,  -6.3077,  -6.6096, -14.9605,  -6.3235, -10.1064,   5.6226,\n",
            "         -15.8638, -13.1383,  -1.4121],\n",
            "        [  5.2071,   5.9648,  -8.4149, -12.7532,  -0.0787,  -8.7443,   3.5905,\n",
            "         -16.7386, -11.6836,  -6.7258],\n",
            "        [  6.2980,  11.0005, -17.0439,  -7.5640,  -0.7971,  -9.0630,  -2.4788,\n",
            "         -10.5959,  -9.3771,   2.8636],\n",
            "        [ 10.6139,   5.5494, -12.3943, -19.2131, -13.6362, -13.1469,   6.7059,\n",
            "         -13.3551, -17.0714,  -9.1835],\n",
            "        [  7.2439,   8.7283, -13.3109, -14.9260,   2.4619,  -7.2852,  -1.0568,\n",
            "         -14.5759, -10.1240,   4.2368],\n",
            "        [  9.8414,   3.1836, -16.6217, -17.1658,   7.4187,  -6.6160,   7.4228,\n",
            "         -13.3170,  -3.1322,  -2.0971],\n",
            "        [  1.8934,  15.9194, -23.6044,  -7.3052,  -4.3729, -10.5866,   1.0390,\n",
            "         -10.5720, -16.9620,   3.4476],\n",
            "        [ 11.6489,  13.7731, -14.4489, -17.2081,  -5.7665, -13.8661,   6.5535,\n",
            "          -6.4964,  -7.8990,  -2.8033],\n",
            "        [ 12.3269,   1.5920,  -8.6540, -22.3742,  -1.8491,  -3.6847,   1.5949,\n",
            "         -18.9836,  -7.4094,  -7.7940],\n",
            "        [  3.8006,   6.2717, -15.6865, -20.3177,  -7.1838,  -3.5848,  -0.6189,\n",
            "         -11.1348, -12.3758,  -3.9186],\n",
            "        [  5.0835,   7.2035, -15.1088, -12.0739,   5.3086, -11.2708,  -2.8451,\n",
            "          -9.1964, -11.8036,  -4.6866],\n",
            "        [ 13.7202,   3.6046,  -9.7766, -24.6667, -11.4899,  -7.1221,   8.4456,\n",
            "         -14.1205, -13.1482,  -3.3967],\n",
            "        [  8.8668,  10.6509, -10.3374, -14.9675,   0.8320,  -7.8538,   5.3459,\n",
            "         -12.4213, -13.2301,  -3.4282],\n",
            "        [  9.1081,   4.0193, -20.5254, -14.2376,  -6.2764, -17.0092,  10.7529,\n",
            "         -19.3207, -18.5431,  -3.4423],\n",
            "        [ 19.2527,  -2.7266, -14.8631, -18.2499,   0.6441, -10.3331,   2.7818,\n",
            "         -17.2086,  -8.9920,   4.0411],\n",
            "        [ 11.7792,  -1.6264,  -9.5885, -15.6766,  -5.4126,  -5.1086,   2.1364,\n",
            "         -16.3391, -19.5109,  -0.5816],\n",
            "        [ -3.6956,  -2.7724, -10.5412, -13.6796, -10.8412,  -6.4618,   6.6648,\n",
            "         -16.4919, -18.5928,  -7.3097],\n",
            "        [  8.6099,   2.0494, -14.3739, -13.6831,  -4.9206, -11.5208,   2.6551,\n",
            "          -5.7994, -10.6386,   7.6758],\n",
            "        [  9.2639,   4.4139, -13.1771, -16.4448,  -2.7827,  -6.8437,   9.4241,\n",
            "         -14.0524, -15.1885,   0.2714],\n",
            "        [ 11.9828,   9.1006, -14.0202, -20.1697,  -5.8243, -15.7333,  -6.8732,\n",
            "         -18.2732, -17.9423,  -1.4738],\n",
            "        [ 11.6297,  12.9525, -14.3751, -19.0521,  -1.9887,   0.2046,   3.4043,\n",
            "         -13.6124, -14.5835,  -0.2394],\n",
            "        [  5.8435,   6.9905,  -8.4519, -11.5076,  -5.4882, -15.2275,  -1.9261,\n",
            "         -12.7150, -18.0502,  -6.3934],\n",
            "        [ 15.8954,  10.8543, -10.0321, -21.3987,  -1.6438,  -9.5077,  -1.0297,\n",
            "         -10.1019, -15.9427,  -4.7766],\n",
            "        [ 11.5424,   9.7394, -20.3692, -20.6941,  -7.2258, -11.1712,  -1.5330,\n",
            "         -10.9182, -19.2997,   0.8109],\n",
            "        [ 15.1542,  -2.1667, -11.5841, -23.6509,  -4.3879, -11.4968,   9.8812,\n",
            "         -11.2382, -16.2480,  -9.6021],\n",
            "        [  0.6329,  -7.3785,  -5.9194,  -9.7961,  -4.1162,  -4.8276,   3.0830,\n",
            "         -15.6213,  -8.5002,  -5.5865],\n",
            "        [  9.0502,   6.6594, -13.6063, -14.4472,  -4.7236, -16.9622,  -1.2488,\n",
            "         -11.3338, -14.8094,  -2.2102],\n",
            "        [  4.6604,   1.9470,  -7.6049, -14.4493,   0.3057,  -2.4472,   5.8305,\n",
            "         -13.5932, -13.5611,  -7.9906],\n",
            "        [ 16.3505,  -5.8724,  -7.3823, -26.2924,  -9.2226,  -7.6625,   8.8733,\n",
            "         -12.1136,  -8.7385,  -9.7414],\n",
            "        [  6.4072,   7.5613,  -9.0618, -29.3536, -12.4559,  -9.4830,   3.4181,\n",
            "         -18.2378, -16.7516,  -2.0932],\n",
            "        [  9.9507,  17.6647, -11.2460, -10.7519,  -0.4476, -11.7558,   5.8083,\n",
            "         -19.6543, -12.6263,  -0.3417],\n",
            "        [  8.4298,   5.7574, -13.4123, -12.8912,  -8.0902, -15.4617,  -5.7321,\n",
            "          -4.8929, -10.9343,   0.0855],\n",
            "        [  7.2024,  24.5344, -18.1611, -17.4094, -15.8155,  -4.1906,  -4.6582,\n",
            "         -11.9099, -10.8677,  -4.4535],\n",
            "        [  9.2172,  -0.3556, -22.1717, -15.0876,   0.2610,  -7.5581,   2.6394,\n",
            "         -16.7951, -14.3792,  -7.8624],\n",
            "        [ -0.0075,  -0.0722, -18.0671, -14.5243,  -1.0408,  -9.2944,   5.5533,\n",
            "         -20.0574, -10.4571,  -5.0891],\n",
            "        [  4.0374,   5.9702, -14.0112, -15.6673, -11.3211, -10.3203,   9.8033,\n",
            "          -8.6815, -14.0653,   5.1797],\n",
            "        [ 11.2019,  14.7997, -20.9784, -10.8570,  -2.0941, -11.6051,   2.3046,\n",
            "         -13.5184, -17.5178,   4.7532],\n",
            "        [  2.4788, -16.4386,  -3.6782,  -9.5174,   2.6590,  -5.3986,   3.3285,\n",
            "         -15.7413, -20.8331,  -0.4217],\n",
            "        [  8.1954,   6.5370, -16.5738, -16.6610, -10.0767, -11.1446,   2.5949,\n",
            "         -17.3549, -15.7830, -10.6205],\n",
            "        [  4.7814,  11.1974, -16.1188, -19.5393,  -6.8067, -11.0902,  -4.7742,\n",
            "         -11.4745, -12.5288,  -7.0865],\n",
            "        [ 11.0843,   4.0264, -11.7002,  -9.3453,  -0.8503,  -7.1121,  -6.8504,\n",
            "         -11.5089, -11.0037,   1.0319],\n",
            "        [  8.8046,   7.0481, -14.5659, -14.9255,  -5.2615,  -6.5697,   8.3776,\n",
            "         -17.7901, -13.5037,  -4.7135]])\n",
            "shape of output:  torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1UGf3ExSvWCn",
        "colab_type": "code",
        "outputId": "3b1b2b84-98ff-46f2-efa3-311aeb1763ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        }
      },
      "cell_type": "code",
      "source": [
        "### define a softmax activation function\n",
        "\n",
        "def softmax(x):\n",
        "  return torch.exp(x) / torch.sum(torch.exp(x), dim=1).view(-1, 1) \n",
        "\n",
        "probabilities = softmax(output_layer) \n",
        "print(\"sum of probabilities tensor:\\n \", probabilities.sum(dim=1))\n",
        "print('')\n",
        "print(\"probabilities tensor:\\n \", probabilities)\n",
        "print('')\n",
        "print(\"shape of probabilities tensor: \", probabilities.shape)\n",
        "print('')\n",
        "print(\"datatype of probabilities tensor: \", type(probabilities)) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum of probabilities tensor:\n",
            "  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000])\n",
            "\n",
            "probabilities tensor:\n",
            "  tensor([[8.3451e-01, 3.7336e-03, 3.2342e-07, 2.2573e-11, 7.2725e-08, 1.4019e-07,\n",
            "         1.6173e-01, 1.4416e-09, 2.2229e-10, 2.3399e-05],\n",
            "        [2.8831e-03, 9.9683e-01, 3.3056e-12, 2.9102e-12, 3.2243e-08, 1.3253e-10,\n",
            "         7.2542e-06, 1.2257e-09, 1.1107e-10, 2.8230e-04],\n",
            "        [9.8824e-01, 3.8697e-08, 2.9534e-09, 3.8615e-11, 2.1382e-06, 2.3984e-09,\n",
            "         1.1754e-02, 1.5992e-11, 1.7748e-10, 6.9408e-07],\n",
            "        [1.5799e-01, 8.4177e-01, 5.7841e-09, 1.0616e-09, 1.5829e-04, 5.8273e-10,\n",
            "         6.3884e-05, 6.7752e-09, 5.5887e-12, 2.2655e-05],\n",
            "        [9.9362e-01, 1.4098e-03, 5.7086e-10, 9.7932e-12, 1.7183e-05, 1.1164e-08,\n",
            "         1.3758e-04, 5.9852e-09, 5.5347e-09, 4.8192e-03],\n",
            "        [9.9101e-01, 8.9924e-03, 4.7666e-14, 5.7417e-17, 2.2994e-14, 3.3404e-10,\n",
            "         5.6243e-07, 7.2413e-13, 1.6707e-11, 1.4762e-11],\n",
            "        [1.0182e-02, 9.8913e-01, 3.5168e-12, 1.2668e-09, 6.4147e-05, 5.5736e-09,\n",
            "         6.2336e-04, 1.0078e-11, 2.8855e-09, 3.5118e-09],\n",
            "        [2.0940e-03, 1.2522e-02, 7.9083e-07, 9.6640e-14, 1.1438e-05, 1.5577e-05,\n",
            "         9.8536e-01, 7.3799e-11, 3.7086e-13, 3.6798e-07],\n",
            "        [2.3703e-01, 7.6258e-01, 1.0166e-14, 9.6306e-16, 8.8699e-12, 1.5313e-11,\n",
            "         6.0239e-09, 2.5179e-10, 4.4100e-12, 3.9209e-04],\n",
            "        [1.3673e-03, 7.0107e-03, 5.9076e-08, 2.6641e-07, 3.5084e-05, 1.3335e-09,\n",
            "         1.6481e-01, 7.7787e-10, 1.1407e-06, 8.2678e-01],\n",
            "        [4.8045e-02, 3.9279e-04, 1.2019e-15, 2.8869e-15, 3.8580e-11, 2.7053e-12,\n",
            "         9.5156e-01, 6.5516e-17, 3.1113e-10, 4.4142e-08],\n",
            "        [2.7624e-03, 9.9546e-01, 1.5089e-12, 2.4507e-10, 9.1573e-06, 3.6030e-08,\n",
            "         1.7381e-03, 7.9771e-10, 4.5917e-12, 2.6977e-05],\n",
            "        [1.8154e-01, 8.0270e-01, 5.0182e-13, 1.2619e-09, 5.8167e-03, 1.2328e-11,\n",
            "         9.7237e-03, 6.8399e-10, 2.8391e-11, 2.2440e-04],\n",
            "        [1.9170e-06, 4.9759e-04, 1.1459e-10, 1.5346e-12, 4.0205e-08, 1.9218e-09,\n",
            "         9.9950e-01, 2.4255e-08, 2.8909e-07, 2.8726e-07],\n",
            "        [1.6482e-01, 8.3518e-01, 1.3572e-10, 1.4571e-14, 2.0463e-08, 1.4553e-07,\n",
            "         2.1341e-06, 3.4749e-12, 1.0095e-12, 2.5626e-07],\n",
            "        [9.8448e-01, 4.9482e-05, 1.9711e-12, 1.2425e-13, 3.9769e-05, 3.6391e-12,\n",
            "         1.5419e-02, 8.2862e-12, 2.0187e-10, 1.1438e-05],\n",
            "        [9.9988e-01, 2.0516e-05, 1.3871e-11, 2.8493e-13, 2.4898e-05, 1.6510e-09,\n",
            "         7.9050e-05, 2.1015e-10, 7.6809e-09, 7.1858e-08],\n",
            "        [9.9537e-01, 3.2708e-04, 4.7317e-09, 2.2449e-08, 9.2196e-06, 3.4060e-07,\n",
            "         4.2799e-03, 2.9501e-07, 6.1172e-10, 9.4489e-06],\n",
            "        [9.9648e-01, 5.1466e-05, 2.6699e-08, 1.0146e-09, 2.2321e-03, 3.8168e-06,\n",
            "         1.1385e-03, 7.9223e-09, 4.9711e-10, 9.7573e-05],\n",
            "        [7.9002e-05, 9.9937e-01, 2.4688e-09, 4.7894e-09, 5.1616e-09, 1.0124e-10,\n",
            "         5.4901e-04, 2.0913e-09, 1.0441e-13, 7.4025e-07],\n",
            "        [1.0416e-01, 8.5717e-01, 8.0906e-10, 1.5606e-07, 3.6180e-02, 1.5239e-05,\n",
            "         1.0131e-05, 6.6424e-10, 4.1958e-08, 2.4647e-03],\n",
            "        [9.9891e-01, 9.0093e-04, 2.3929e-17, 3.4674e-16, 2.1646e-13, 4.0146e-12,\n",
            "         9.7412e-06, 4.9219e-13, 1.9004e-09, 1.7842e-04],\n",
            "        [3.5662e-04, 6.5798e-06, 4.8652e-06, 1.1491e-09, 6.4769e-06, 1.4739e-07,\n",
            "         9.9875e-01, 4.6566e-10, 7.1075e-09, 8.7971e-04],\n",
            "        [2.9967e-01, 6.3930e-01, 3.6365e-07, 4.7487e-09, 1.5172e-03, 2.6160e-07,\n",
            "         5.9505e-02, 8.8256e-11, 1.3840e-08, 1.9691e-06],\n",
            "        [8.9889e-03, 9.9071e-01, 6.5529e-13, 8.5806e-09, 7.4532e-06, 1.9164e-09,\n",
            "         1.3867e-06, 4.1378e-10, 1.3998e-09, 2.8984e-04],\n",
            "        [9.7428e-01, 6.1544e-03, 9.9161e-11, 1.0839e-13, 2.8643e-11, 4.6718e-11,\n",
            "         1.9565e-02, 3.7939e-11, 9.2285e-13, 2.4592e-09],\n",
            "        [1.8280e-01, 8.0659e-01, 2.1633e-10, 4.3023e-11, 1.5318e-03, 8.9552e-08,\n",
            "         4.5396e-05, 6.1064e-11, 5.2386e-09, 9.0367e-03],\n",
            "        [8.4816e-01, 1.0890e-03, 2.7270e-12, 1.5826e-12, 7.5217e-02, 6.0409e-08,\n",
            "         7.5527e-02, 7.4290e-11, 1.9684e-06, 5.5419e-06],\n",
            "        [8.1013e-07, 9.9999e-01, 6.8390e-18, 8.1972e-11, 1.5387e-09, 3.0801e-12,\n",
            "         3.4476e-07, 3.1254e-12, 5.2455e-15, 3.8330e-06],\n",
            "        [1.0670e-01, 8.9264e-01, 4.9435e-13, 3.1313e-14, 2.9158e-09, 8.8536e-13,\n",
            "         6.5349e-04, 1.4052e-09, 3.4565e-10, 5.6446e-08],\n",
            "        [9.9996e-01, 2.1771e-05, 7.7286e-10, 8.5015e-16, 6.9736e-07, 1.1124e-07,\n",
            "         2.1834e-05, 2.5237e-14, 2.6830e-09, 1.8265e-09],\n",
            "        [7.7835e-02, 9.2114e-01, 2.6792e-10, 2.6104e-12, 1.3203e-06, 4.8272e-05,\n",
            "         9.3712e-04, 2.5397e-08, 7.3425e-09, 3.4575e-05],\n",
            "        [9.4483e-02, 7.8715e-01, 1.6067e-10, 3.3418e-09, 1.1833e-01, 7.4602e-09,\n",
            "         3.4042e-05, 5.9382e-08, 4.3789e-09, 5.3981e-06],\n",
            "        [9.9487e-01, 4.0236e-05, 6.2117e-11, 2.1209e-17, 1.1198e-11, 8.8321e-10,\n",
            "         5.0935e-03, 8.0667e-13, 2.1329e-12, 3.6641e-08],\n",
            "        [1.4318e-01, 8.5254e-01, 6.5407e-10, 6.3792e-12, 4.6390e-05, 7.8383e-09,\n",
            "         4.2344e-03, 8.1389e-11, 3.6253e-11, 6.5500e-07],\n",
            "        [1.6165e-01, 9.9664e-04, 2.1823e-14, 1.1740e-11, 3.3666e-08, 7.3451e-13,\n",
            "         8.3735e-01, 7.2799e-14, 1.5842e-13, 5.7281e-07],\n",
            "        [1.0000e+00, 2.8479e-10, 1.5265e-15, 5.1621e-17, 8.2869e-09, 1.4159e-13,\n",
            "         7.0275e-08, 1.4623e-16, 5.4138e-13, 2.4757e-07],\n",
            "        [9.9993e-01, 1.5066e-06, 5.2495e-10, 1.1915e-12, 3.4173e-08, 4.6311e-08,\n",
            "         6.4891e-05, 6.1430e-13, 2.5756e-14, 4.2832e-06],\n",
            "        [3.1659e-05, 7.9702e-05, 3.3690e-08, 1.4605e-09, 2.4957e-08, 1.9914e-06,\n",
            "         9.9989e-01, 8.7732e-11, 1.0733e-11, 8.5294e-07],\n",
            "        [7.1585e-01, 1.0130e-03, 7.4660e-11, 1.4896e-10, 9.5188e-07, 1.2946e-09,\n",
            "         1.8564e-03, 3.9530e-07, 3.1282e-09, 2.8128e-01],\n",
            "        [4.5836e-01, 3.5881e-03, 8.2258e-11, 3.1336e-12, 2.6879e-06, 4.6318e-08,\n",
            "         5.3799e-01, 3.4280e-11, 1.1006e-11, 5.6989e-05],\n",
            "        [9.4696e-01, 5.3040e-02, 4.8237e-12, 1.0297e-14, 1.7490e-08, 8.6973e-13,\n",
            "         6.1272e-09, 6.8598e-14, 9.5509e-14, 1.3558e-06],\n",
            "        [2.1035e-01, 7.8959e-01, 1.0695e-12, 9.9532e-15, 2.5617e-07, 2.2965e-06,\n",
            "         5.6321e-05, 2.2931e-12, 8.6832e-13, 1.4731e-06],\n",
            "        [2.4101e-01, 7.5888e-01, 1.4914e-07, 7.0234e-09, 2.8889e-06, 1.7022e-10,\n",
            "         1.0180e-04, 2.0997e-09, 1.0119e-11, 1.1684e-06],\n",
            "        [9.9357e-01, 6.4253e-03, 5.4580e-12, 6.3184e-17, 2.3991e-08, 9.2211e-12,\n",
            "         4.4335e-08, 5.0900e-12, 1.4795e-14, 1.0459e-09],\n",
            "        [8.5851e-01, 1.4147e-01, 1.1876e-14, 8.5818e-15, 6.0644e-09, 1.1731e-10,\n",
            "         1.7994e-06, 1.5108e-10, 3.4609e-14, 1.8754e-05],\n",
            "        [9.9490e-01, 2.9884e-08, 2.4294e-12, 1.3963e-17, 3.2417e-09, 2.6509e-12,\n",
            "         5.1023e-03, 3.4333e-12, 2.2908e-14, 1.7631e-11],\n",
            "        [7.9329e-02, 2.6308e-05, 1.1318e-04, 2.3452e-06, 6.8692e-04, 3.3724e-04,\n",
            "         9.1934e-01, 6.9228e-09, 8.5698e-06, 1.5789e-04],\n",
            "        [9.1608e-01, 8.3877e-02, 1.3254e-10, 5.7170e-11, 9.5507e-07, 4.6226e-12,\n",
            "         3.0841e-05, 1.2862e-09, 3.9797e-11, 1.1792e-05],\n",
            "        [2.3245e-01, 1.5413e-02, 1.0953e-06, 1.1670e-09, 2.9861e-03, 1.9033e-04,\n",
            "         7.4896e-01, 2.7471e-09, 2.8367e-09, 7.4480e-07],\n",
            "        [9.9943e-01, 2.2309e-10, 4.9288e-11, 3.0210e-19, 7.8250e-12, 3.7242e-11,\n",
            "         5.6553e-04, 4.3448e-13, 1.2698e-11, 4.6579e-12],\n",
            "        [2.3688e-01, 7.5115e-01, 4.5335e-08, 6.9791e-17, 1.5218e-09, 2.9750e-08,\n",
            "         1.1923e-02, 4.6917e-12, 2.0738e-11, 4.8179e-05],\n",
            "        [4.4629e-04, 9.9955e-01, 2.7799e-13, 4.5565e-13, 1.3606e-08, 1.6696e-13,\n",
            "         7.0895e-06, 6.1991e-17, 6.9913e-14, 1.5126e-08],\n",
            "        [9.3516e-01, 6.4611e-02, 3.0551e-10, 5.1443e-10, 6.2567e-08, 3.9352e-11,\n",
            "         6.6139e-07, 1.5308e-06, 3.6406e-09, 2.2234e-04],\n",
            "        [2.9702e-08, 1.0000e+00, 2.8679e-19, 6.0815e-19, 2.9939e-18, 3.3486e-13,\n",
            "         2.0981e-13, 1.4874e-16, 4.2176e-16, 2.5744e-13],\n",
            "        [9.9841e-01, 6.9487e-05, 2.3296e-14, 2.7788e-11, 1.2872e-04, 5.1745e-08,\n",
            "         1.3887e-03, 5.0384e-12, 5.6430e-11, 3.8169e-08],\n",
            "        [3.8120e-03, 3.5732e-03, 5.4697e-11, 1.8905e-09, 1.3564e-03, 3.5311e-07,\n",
            "         9.9123e-01, 7.4744e-12, 1.1040e-07, 2.3672e-05],\n",
            "        [3.0278e-03, 2.0918e-02, 4.3925e-11, 8.3848e-12, 6.4717e-10, 1.7605e-09,\n",
            "         9.6657e-01, 9.0646e-09, 4.1613e-11, 9.4891e-03],\n",
            "        [2.6653e-02, 9.7330e-01, 2.8185e-16, 7.0096e-12, 4.4807e-08, 3.3174e-12,\n",
            "         3.6449e-06, 4.8960e-13, 8.9727e-15, 4.2181e-05],\n",
            "        [2.1767e-01, 1.3247e-09, 4.6117e-04, 1.3426e-06, 2.6066e-01, 8.2548e-05,\n",
            "         5.0916e-01, 2.6604e-09, 1.6352e-11, 1.1971e-02],\n",
            "        [8.3741e-01, 1.5949e-01, 1.4650e-11, 1.3427e-11, 9.7159e-09, 3.3397e-09,\n",
            "         3.0952e-03, 6.7079e-12, 3.2304e-11, 5.6406e-09],\n",
            "        [1.6324e-03, 9.9837e-01, 1.3678e-12, 4.4721e-14, 1.5143e-08, 2.0887e-10,\n",
            "         1.1559e-07, 1.4223e-10, 4.9558e-11, 1.1447e-08],\n",
            "        [9.9909e-01, 8.5980e-04, 1.2718e-10, 1.3400e-09, 6.5532e-06, 1.2502e-08,\n",
            "         1.6243e-08, 1.5399e-10, 2.5521e-10, 4.3041e-05],\n",
            "        [5.4790e-01, 9.4594e-02, 3.8819e-11, 2.7092e-11, 4.2644e-07, 1.1528e-07,\n",
            "         3.5750e-01, 1.5445e-12, 1.1228e-10, 7.3768e-07]])\n",
            "\n",
            "shape of probabilities tensor:  torch.Size([64, 10])\n",
            "\n",
            "datatype of probabilities tensor:  <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0EwEdbCTjjvE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This section builds the same network as above,**\n",
        "\n",
        "**But it uses the PyTorch 'nn' module for simplicity.**"
      ]
    },
    {
      "metadata": {
        "id": "g28YNcDg190G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### build a network with torch.nn\n",
        "\n",
        "### import torch.nn\n",
        "\n",
        "from torch import  nn\n",
        "import torch.nn.functional as F "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvtIkKdFQFEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### define a class named 'Network'\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "       super().__init__()\n",
        "    \n",
        "       # inputs to hidden layer\n",
        "       self.hidden1 = nn.Linear(784, 128)\n",
        "       self.hidden2 = nn.Linear(128, 64) \n",
        "       # output layer \n",
        "       self.output = nn.Linear(64, 10) \n",
        "    \n",
        "       # define sigmoid activation function and softmax output function\n",
        "       self.relu = nn.ReLU()  \n",
        "       self.sigmoid = nn.Sigmoid() \n",
        "       self.softmax = nn.Softmax(dim=1) \n",
        "        \n",
        "        \n",
        "    ### pass the input tensor (x) through each operation\n",
        "    def forward(self, x):\n",
        "        # hidden layer with sigmoid activtion function \n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        # output layer with softmax activation function\n",
        "        x = F.softmax(self.output(x), dim=1) \n",
        "        \n",
        "        return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQHuxezTYMjv",
        "colab_type": "code",
        "outputId": "88ec1364-4206-4e2a-e065-46a48e8e2bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "### create a network object \n",
        "\n",
        "model = Network() \n",
        "model "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              "  (softmax): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "1f80eTLL2nDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initializing weights and biases.**"
      ]
    },
    {
      "metadata": {
        "id": "Ae8V-W8o2vaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "27ef36dc-ea6c-4ff6-9b3a-b2af383a04a0"
      },
      "cell_type": "code",
      "source": [
        "### print the weights and biases (automatically initialized)\n",
        "\n",
        "print(\"weights:\\n \", model.hidden1.weight)\n",
        "print(\"biases:\\n \", model.hidden1.bias)   "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights:\n",
            "  Parameter containing:\n",
            "tensor([[ 0.0255,  0.0229,  0.0187,  ..., -0.0130,  0.0143,  0.0020],\n",
            "        [-0.0055, -0.0174, -0.0101,  ...,  0.0004, -0.0329,  0.0119],\n",
            "        [ 0.0282,  0.0242, -0.0055,  ...,  0.0280,  0.0284,  0.0189],\n",
            "        ...,\n",
            "        [-0.0178,  0.0170,  0.0180,  ..., -0.0344,  0.0007, -0.0059],\n",
            "        [ 0.0053, -0.0077,  0.0318,  ...,  0.0269,  0.0018,  0.0245],\n",
            "        [ 0.0131,  0.0036,  0.0071,  ..., -0.0112,  0.0225, -0.0024]],\n",
            "       requires_grad=True)\n",
            "biases:\n",
            "  Parameter containing:\n",
            "tensor([ 0.0201, -0.0248, -0.0239,  0.0185, -0.0203, -0.0031, -0.0180,  0.0072,\n",
            "        -0.0204, -0.0347,  0.0011,  0.0270,  0.0061,  0.0100,  0.0101, -0.0357,\n",
            "        -0.0351,  0.0348, -0.0250, -0.0120, -0.0296, -0.0308, -0.0083, -0.0178,\n",
            "        -0.0179,  0.0053, -0.0309, -0.0032,  0.0327,  0.0050,  0.0069,  0.0282,\n",
            "         0.0079, -0.0036,  0.0298, -0.0144,  0.0302, -0.0007,  0.0184, -0.0126,\n",
            "        -0.0325, -0.0251, -0.0055,  0.0289,  0.0193,  0.0056,  0.0008,  0.0037,\n",
            "         0.0322, -0.0338, -0.0235,  0.0160, -0.0276, -0.0044, -0.0089, -0.0247,\n",
            "        -0.0353, -0.0260, -0.0013, -0.0259, -0.0353, -0.0206,  0.0028, -0.0314,\n",
            "         0.0030,  0.0279,  0.0269,  0.0138, -0.0037,  0.0073, -0.0143, -0.0088,\n",
            "        -0.0117,  0.0354, -0.0277,  0.0084,  0.0051,  0.0207, -0.0036, -0.0108,\n",
            "         0.0088, -0.0240,  0.0269, -0.0304,  0.0259, -0.0095, -0.0167, -0.0249,\n",
            "         0.0225,  0.0244, -0.0025, -0.0272, -0.0152, -0.0059,  0.0096, -0.0176,\n",
            "         0.0108, -0.0250, -0.0060,  0.0122,  0.0273, -0.0280, -0.0194,  0.0204,\n",
            "        -0.0063, -0.0340, -0.0138,  0.0010,  0.0310,  0.0279, -0.0006, -0.0084,\n",
            "        -0.0004, -0.0049, -0.0327,  0.0054,  0.0161, -0.0082,  0.0009, -0.0154,\n",
            "        -0.0193,  0.0236, -0.0242, -0.0037, -0.0047, -0.0063, -0.0086, -0.0154],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "chquHnIH47gF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "800948d5-b57c-4092-e1c8-a9f961d1c18a"
      },
      "cell_type": "code",
      "source": [
        "### print the weights (custom initialization)\n",
        "\n",
        "model.hidden1.weight.data.normal_(std=0.01) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0055,  0.0144, -0.0033,  ...,  0.0068,  0.0100,  0.0049],\n",
              "        [ 0.0031, -0.0180,  0.0012,  ..., -0.0051, -0.0051,  0.0225],\n",
              "        [ 0.0005, -0.0084,  0.0114,  ...,  0.0011,  0.0056,  0.0099],\n",
              "        ...,\n",
              "        [ 0.0024,  0.0123,  0.0023,  ..., -0.0066, -0.0004,  0.0001],\n",
              "        [-0.0222,  0.0035, -0.0117,  ..., -0.0026, -0.0123,  0.0084],\n",
              "        [-0.0100,  0.0004,  0.0121,  ...,  0.0056, -0.0098, -0.0033]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "D9THnjaK542P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2d9e3760-5725-49d2-d4fd-35c5da184f1b"
      },
      "cell_type": "code",
      "source": [
        "### print the biases (custom initialization) \n",
        "\n",
        "model.hidden1.bias.data.fill_(0)  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "PEWAeTiq6peM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Forward pass.**\n",
        "\n",
        "**Pass an image into the network.**"
      ]
    },
    {
      "metadata": {
        "id": "LcLTum_47Cbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1914
        },
        "outputId": "a682ce2d-5b2d-4785-ef1d-4d2f12c29848"
      },
      "cell_type": "code",
      "source": [
        "### resize the image into a 1D vector\n",
        "\n",
        "images.resize_(64, 1, 784) \n",
        "\n",
        "### forward pass through the network\n",
        "\n",
        "img_idx = 0\n",
        "ps = model.forward(images[img_idx,:])\n",
        "print(\"ps: \", ps)\n",
        "print('')\n",
        "\n",
        "img = images[img_idx] \n",
        "print(\"img: \", img) \n",
        "\n",
        "helper.view_classify(img.view(1, 28, 28), ps)  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ps:  tensor([[0.1033, 0.1001, 0.1047, 0.0951, 0.0973, 0.0962, 0.1002, 0.1030, 0.1063,\n",
            "         0.0938]], grad_fn=<SoftmaxBackward>)\n",
            "\n",
            "img:  tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -0.7490,  0.0118,  0.9843,  1.0000,  0.9843,  0.8196,\n",
            "         -0.1451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9922,  0.0510,  0.7804,  0.9765,\n",
            "          0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.2235, -0.9216, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9529,\n",
            "         -0.0588,  0.9765,  0.9765,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,\n",
            "          0.9765,  0.9843, -0.5608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.0588,  0.9765,  0.9765,  0.9765,  0.9765,\n",
            "          0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.9843,  0.4039, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4196,\n",
            "          0.9765,  0.9765,  0.9765,  0.9765,  0.4824,  0.1294,  0.8588,  0.9765,\n",
            "          0.9765,  0.9843,  0.4039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000,  0.0902,  0.9765,  0.9765,  0.1608, -0.7255,\n",
            "         -0.8824, -1.0000,  0.7020,  0.9765,  0.9765,  0.9843,  0.0824, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,\n",
            "         -0.4431, -0.4431, -0.9216, -1.0000, -1.0000, -1.0000,  0.7020,  0.9765,\n",
            "          0.9765,  0.3725, -0.9216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -0.5059,  0.8588,  0.9765,  0.9765,  0.1294, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.5059,  1.0000,  0.9843,  0.9843,\n",
            "          0.9843, -0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,\n",
            "          0.3725,  0.9843,  0.9765,  0.9765,  0.6471, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -0.9137,  0.1765,  0.9765,  0.9843,  0.9765,  0.7725,\n",
            "         -0.7569, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4275,  0.9765,\n",
            "          0.9765,  0.9843,  0.9765,  0.0353, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000,  0.0588,  0.9843,  0.9843,  1.0000,  0.7412, -0.6784,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.7804,  0.9765,\n",
            "          0.9765,  0.9843, -0.5686, -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,\n",
            "         -0.4275, -0.4275, -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.9922,  0.6549,  0.9765,  0.9765,  0.9765,  0.8588, -0.7647, -1.0000,\n",
            "         -0.8353, -0.7098, -0.7098,  0.1765,  0.9765,  0.9843,  0.4039, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9922,  0.9765,  0.9765,  0.9765,\n",
            "          0.9765,  0.6235,  0.1294,  0.1294,  0.6157,  0.9843,  0.9765,  0.9765,\n",
            "          0.9765,  0.9843, -0.2392, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.9922,  0.9843,  0.9843,  0.9843,  0.9843,  1.0000,  0.9843,  0.9843,\n",
            "          0.9843,  1.0000,  0.9843,  0.9843,  0.4980, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9922,  0.9765,  0.9765,  0.9765,\n",
            "          0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.9843,  0.9765,  0.4039,\n",
            "         -0.8824, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.9922,  0.0431,  0.7725,  0.9765,  0.9765,  0.9843,  0.9765,  0.9294,\n",
            "          0.6863,  0.2078, -0.4431, -0.8431, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7569, -0.1529,\n",
            "          0.4902, -0.1529, -0.1529, -0.2784, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d256b1cd6bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'view_classify'"
          ]
        }
      ]
    }
  ]
}